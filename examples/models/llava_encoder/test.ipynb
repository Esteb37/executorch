{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "from torch import nn\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, tokenizer_image_token\n",
    "from llava.eval.run_llava import eval_model, load_images, process_images\n",
    "# from executorch.exir import EdgeProgramManager, ExecutorchProgramManager, to_edge\"\n",
    "\n",
    "from llava.model.multimodal_encoder.builder import build_vision_tower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "image = Image.open(requests.get('https://llava-vl.github.io/static/images/view.jpg', stream=True).raw)\n",
    "# temp_file = \"/Users/larryliu/Downloads/pyturkeys.jpg\"\n",
    "temp_file = \"./view.jpg\"\n",
    "image.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 667, 1000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms._functional_tensor import resize\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "# from helpers import plot\n",
    "\n",
    "from torch.export import export\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torch.export import Dim\n",
    "from dataclasses import dataclass\n",
    "\n",
    "imagr = torchvision.io.read_image(temp_file)\n",
    "print(imagr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[166, 166, 166,  ..., 150, 149, 149],\n",
      "         [166, 166, 166,  ..., 150, 149, 149],\n",
      "         [166, 166, 166,  ..., 150, 150, 149],\n",
      "         ...,\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29],\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29],\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29]],\n",
      "\n",
      "        [[193, 193, 193,  ..., 181, 180, 180],\n",
      "         [193, 193, 193,  ..., 181, 180, 180],\n",
      "         [193, 193, 193,  ..., 181, 181, 180],\n",
      "         ...,\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51],\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51],\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51]],\n",
      "\n",
      "        [[220, 220, 220,  ..., 210, 209, 209],\n",
      "         [220, 220, 220,  ..., 210, 209, 209],\n",
      "         [220, 220, 220,  ..., 210, 210, 209],\n",
      "         ...,\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64],\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64],\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(imagr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "prompt = \"What are the things I should be cautious about when I visit here?\"\n",
    "image_files = [temp_file]  # IMG_3997\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"query\": prompt,\n",
    "    \"conv_mode\": None,\n",
    "    \"image_file\": image_files[0],\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512\n",
    "})()\n",
    "\n",
    "model_name = get_model_name_from_path(args.model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    args.model_path, args.model_base, model_name, device_map=\"cpu\", device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.constants import (\n",
    "    IGNORE_INDEX,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "import re\n",
    "\n",
    "def get_prompt(query: str, mm_use_im_start_end: bool, model_name: str) -> str:\n",
    "    qs = query\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in qs:\n",
    "        if mm_use_im_start_end:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "        else:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "    else:\n",
    "        if mm_use_im_start_end:\n",
    "            qs = image_token_se + \"\\n\" + qs\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "    def get_conv_mode(model_name: str) -> str:\n",
    "        if \"llama-2\" in model_name.lower():\n",
    "            conv_mode = \"llava_llama_2\"\n",
    "        elif \"mistral\" in model_name.lower():\n",
    "            conv_mode = \"mistral_instruct\"\n",
    "        elif \"v1.6-34b\" in model_name.lower():\n",
    "            conv_mode = \"chatml_direct\"\n",
    "        elif \"v1\" in model_name.lower():\n",
    "            conv_mode = \"llava_v1\"\n",
    "        elif \"mpt\" in model_name.lower():\n",
    "            conv_mode = \"mpt\"\n",
    "        else:\n",
    "            conv_mode = \"llava_v0\"\n",
    "        return conv_mode\n",
    "    conv = conv_templates[get_conv_mode(model_name)].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_tensor(args) -> torch.Tensor:\n",
    "    image_files = args.image_file.split(args.sep)\n",
    "    images = load_images(image_files)\n",
    "    image_sizes = [x.size for x in images]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "    return image_sizes, images_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 166.5\n",
      "0 167 0 166\n",
      "torch.Size([3, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "img = imagr\n",
    "w = max(img.shape[1], img.shape[2])\n",
    "v_padding = (w - img.shape[1]) / 2\n",
    "h_padding = (w - img.shape[2]) / 2\n",
    "print(h_padding, v_padding)\n",
    "l_pad = int(h_padding if h_padding % 1 == 0 else h_padding+0.5)\n",
    "t_pad = int(v_padding if v_padding % 1 == 0 else v_padding+0.5)\n",
    "r_pad = int(h_padding if h_padding % 1 == 0 else h_padding-0.5)\n",
    "b_pad = int(v_padding if v_padding % 1 == 0 else v_padding-0.5)\n",
    "print(l_pad, t_pad, r_pad, b_pad)\n",
    "padding = torchvision.transforms.v2.Pad(padding=(l_pad, t_pad, r_pad, b_pad), fill=tuple(int(x*255) for x in image_processor.image_mean))\n",
    "padded = padding(img)\n",
    "print(padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "336\n"
     ]
    }
   ],
   "source": [
    "print(image_processor.crop_size['height'])\n",
    "print(image_processor.crop_size['width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 336, 336])\n",
      "torch.Size([3, 1000, 1000])\n",
      "tensor([[[0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         ...,\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784]],\n",
      "\n",
      "        [[0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         ...,\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549]],\n",
      "\n",
      "        [[0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         ...,\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078]]])\n",
      "torch.Size([3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "# interpolation=\"bicubic\" -> PIL image.resize(resample=3)\n",
    "resized = resize(padded, size=[image_processor.crop_size['height'], image_processor.crop_size['width']], interpolation=\"bicubic\")\n",
    "print(resized.shape)\n",
    "cropped = torchvision.transforms.v2.functional.center_crop(img, output_size=[w, w])\n",
    "print(cropped.shape)\n",
    "scaled = resized * image_processor.rescale_factor\n",
    "print(scaled)\n",
    "normed = torchvision.transforms.v2.functional.normalize(scaled, image_processor.image_mean, image_processor.image_std)\n",
    "print(normed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.transforms.functional.to_pil_image(resized)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48145466, 0.4578275, 0.40821073]\n",
      "[0.26862954, 0.26130258, 0.27577711]\n",
      "0.00392156862745098\n"
     ]
    }
   ],
   "source": [
    "print(image_processor.image_mean)\n",
    "print(image_processor.image_std)\n",
    "print(image_processor.rescale_factor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking inside the model object from load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llava.model.language_model.llava_llama.LlavaLlamaForCausalLM"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llava.model.language_model.llava_llama.LlavaLlamaModel"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.get_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llava-v1.5-7b'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_name_from_path(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_prompt(args.query, model.config.mm_use_im_start_end, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "What are the things I should be cautious about when I visit here? ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes, image_tensor = get_image_tensor(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cpu()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   319, 13563,  1546,   263, 12758,  5199,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  5199, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  5618,   526,\n",
      "           278,  2712,   306,   881,   367,   274,  1300,  2738,  1048,   746,\n",
      "           306,  6493,  1244, 29973,   319,  1799,  9047, 13566, 29901]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.llava_arch import LlavaMetaForCausalLM\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PreprocessConfig:\n",
    "    crop_size: dict\n",
    "    image_mean: list[float]\n",
    "    image_std: list[float]\n",
    "    rescale_factor: float\n",
    "\n",
    "class TextImageTokenEmbedding(torch.nn.Module):\n",
    "    def __init__(self, llava_model: LlavaMetaForCausalLM, config: PreprocessConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model_ = llava_model\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model_.get_model()\n",
    "\n",
    "    def encode_images(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        images = images.to(dtype=torch.float16)\n",
    "        image_features = self.get_model().get_vision_tower()(images)\n",
    "        image_features = self.get_model().mm_projector(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def image_preprocess(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        w = max(img.shape[1], img.shape[2])\n",
    "        # pad the image with median rgb value, to make a square\n",
    "        v_padding = (w - img.shape[1]) / 2\n",
    "        h_padding = (w - img.shape[2]) / 2\n",
    "        l_pad = int(h_padding if h_padding % 1 == 0 else h_padding+0.5)\n",
    "        t_pad = int(v_padding if v_padding % 1 == 0 else v_padding+0.5)\n",
    "        r_pad = int(h_padding if h_padding % 1 == 0 else h_padding-0.5)\n",
    "        b_pad = int(v_padding if v_padding % 1 == 0 else v_padding-0.5)\n",
    "        padding = torchvision.transforms.v2.Pad(padding=(l_pad, t_pad, r_pad, b_pad), fill=tuple(int(x*255) for x in image_processor.image_mean))\n",
    "        padded = padding(img)\n",
    "        # here padded shape should be max(h, w) x max(h, w)\n",
    "        resized = resize(padded, size=[image_processor.crop_size['height'], image_processor.crop_size['width']], interpolation=\"bicubic\")\n",
    "        torch._check(resized.size(1) == self.config.crop_size['height'])\n",
    "        torch._check(resized.size(2) == self.config.crop_size['width'])\n",
    "        # print(resized.shape)\n",
    "        # cropped = torchvision.transforms.v2.functional.center_crop(img, output_size=[w, w])\n",
    "        # print(cropped.shape)\n",
    "        scaled = resized * self.config.rescale_factor\n",
    "        # print(scaled)\n",
    "        normed = torchvision.transforms.v2.functional.normalize(scaled, self.config.image_mean, self.config.image_std)\n",
    "        # print(normed)\n",
    "        return normed.unsqueeze(0)\n",
    "    \n",
    "    def prepare_inputs_labels_for_multimodal_one_image(self, input_ids: torch.Tensor, images: torch.Tensor) -> torch.Tensor:\n",
    "        assert isinstance(input_ids, torch.Tensor), f\"Expecting input_ids to be a tensor, got {input_ids}\"\n",
    "        assert input_ids.shape[0] == 1, f\"Expecting input_ids to be of shape [1, num_tokens], got {input_ids.shape}\"\n",
    "        input_ids = input_ids.squeeze(0)\n",
    "        # preprocessed_img = self.image_preprocess(imagt)\n",
    "        # preprocessed_img = torch.unsqueeze(preprocessed_img, dim=0)\n",
    "\n",
    "        index = torch.where(input_ids == IMAGE_TOKEN_INDEX)[0]\n",
    "        input_before_img = self.get_model().embed_tokens(input_ids[:index]).unsqueeze(0)\n",
    "        input_after_img = self.get_model().embed_tokens(input_ids[index+1:]).unsqueeze(0)\n",
    "\n",
    "        image_embeds = self.encode_images(images)\n",
    "        # new_input_embeds = torch.cat(input_embeds, image_embeds)\n",
    "        result = torch.cat((input_before_img, image_embeds, input_after_img), dim=1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, images: torch.Tensor) -> torch.Tensor:\n",
    "        preprocessed_img = self.image_preprocess(images)\n",
    "        embeds = self.prepare_inputs_labels_for_multimodal_one_image(input_ids, preprocessed_img)\n",
    "        return self.get_model().forward(inputs_embeds=embeds, return_dict=False, use_cache=False, output_hidden_states=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_config = PreprocessConfig(image_processor.crop_size, image_processor.image_mean, image_processor.image_std,\n",
    "                              image_processor.rescale_factor)\n",
    "embedding = TextImageTokenEmbedding(model, pre_config)\n",
    "preprocessed_img = embedding.image_preprocess(imagr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, compare, _ = model.prepare_inputs_labels_for_multimodal(input_ids, None, None, None, None, image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(preprocessed_img.to(dtype=torch.float16), image_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.2411, -0.1285,  0.1166,  ...,  0.0782, -0.1182,  0.1437],\n",
      "         [ 0.4277, -0.7944, -2.2910,  ...,  1.0479,  1.6650,  0.4583],\n",
      "         [ 1.0410,  0.9482,  1.3701,  ..., -1.0566, -0.7695,  0.1592],\n",
      "         ...,\n",
      "         [-2.0820,  0.1720,  0.2291,  ...,  0.8159,  0.0366,  0.0337],\n",
      "         [-1.1045,  1.8584, -1.5156,  ..., -0.8760,  1.7559,  0.9385],\n",
      "         [ 0.8013,  0.1904, -0.2949,  ...,  0.3408,  0.7412,  1.2500]]],\n",
      "       dtype=torch.float16, grad_fn=<MulBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "hidden_state = embedding.forward(input_ids, imagr)\n",
    "print(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 634, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_state[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
