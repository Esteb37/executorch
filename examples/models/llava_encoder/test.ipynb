{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "from torch import nn\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, tokenizer_image_token\n",
    "from llava.eval.run_llava import eval_model, load_images, process_images\n",
    "# from executorch.exir import EdgeProgramManager, ExecutorchProgramManager, to_edge\n",
    "\n",
    "from llava.model.multimodal_encoder.builder import build_vision_tower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "image = Image.open(requests.get('https://llava-vl.github.io/static/images/view.jpg', stream=True).raw)\n",
    "# temp_file = \"/Users/larryliu/Downloads/pyturkeys.jpg\"\n",
    "temp_file = \"./view.jpg\"\n",
    "image.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.13it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "prompt = \"What are the things I should be cautious about when I visit here?\"\n",
    "image_files = [temp_file]  # IMG_3997\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"query\": prompt,\n",
    "    \"conv_mode\": None,\n",
    "    \"image_file\": image_files[0],\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512\n",
    "})()\n",
    "\n",
    "model_name = get_model_name_from_path(args.model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    args.model_path, args.model_base, model_name, device_map=\"cpu\", device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 667, 1000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms._functional_tensor import resize\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "# from helpers import plot\n",
    "\n",
    "from torch.export import export\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torch.export import Dim\n",
    "from dataclasses import dataclass\n",
    "\n",
    "imagr = torchvision.io.read_image(image_files[0])\n",
    "imagt = imagr.to(dtype=torch.float16)\n",
    "print(imagt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PreprocessConfig:\n",
    "    crop_size: dict\n",
    "    image_mean: list[float]\n",
    "    image_std: list[float]\n",
    "    rescale_factor: float\n",
    "\n",
    "\n",
    "class Preprocess(nn.Module):\n",
    "    def __init__(self, config: PreprocessConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, img):\n",
    "        w = max(img.shape[1], img.shape[2])\n",
    "        padded = torchvision.transforms.v2.functional.center_crop(img, output_size=[w, w])\n",
    "        resized = resize(padded, size=[self.config.crop_size['height'], self.config.crop_size['width']])\n",
    "        torch._check(resized.size(1) == self.config.crop_size['height'])\n",
    "        torch._check(resized.size(2) == self.config.crop_size['width'])\n",
    "        scaled = resized * self.config.rescale_factor\n",
    "        normed = torchvision.transforms.v2.functional.normalize(scaled, self.config.image_mean, self.config.image_std)\n",
    "        return normed\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, llava_model):\n",
    "        super().__init__()\n",
    "        self.model_ = llava_model\n",
    "\n",
    "    def forward(self, images_tensor):\n",
    "        features = self.model_.get_model().get_vision_tower()(images_tensor)\n",
    "        features = self.model_.get_model().mm_projector(features)\n",
    "        return features\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self, preprocessor, encoder):\n",
    "        super().__init__()\n",
    "        self.pre_processor_ = preprocessor\n",
    "        self.encoder_ = encoder\n",
    "\n",
    "    def forward(self, image):\n",
    "        processed = self.pre_processor_(image)\n",
    "        processed = torch.unsqueeze(processed, dim=0)\n",
    "        features = self.encoder_(processed)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[166, 166, 166,  ..., 150, 149, 149],\n",
      "         [166, 166, 166,  ..., 150, 149, 149],\n",
      "         [166, 166, 166,  ..., 150, 150, 149],\n",
      "         ...,\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29],\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29],\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29]],\n",
      "\n",
      "        [[193, 193, 193,  ..., 181, 180, 180],\n",
      "         [193, 193, 193,  ..., 181, 180, 180],\n",
      "         [193, 193, 193,  ..., 181, 181, 180],\n",
      "         ...,\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51],\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51],\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51]],\n",
      "\n",
      "        [[220, 220, 220,  ..., 210, 209, 209],\n",
      "         [220, 220, 220,  ..., 210, 209, 209],\n",
      "         [220, 220, 220,  ..., 210, 210, 209],\n",
      "         ...,\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64],\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64],\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(imagr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 667, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(imagt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imagr\n",
    "w = max(img.shape[1], img.shape[2])\n",
    "h_padding = (w - img.shape[1]) / 2\n",
    "v_padding = (w - img.shape[2]) / 2\n",
    "l_pad = int(h_padding if h_padding % 1 == 0 else h_padding+0.5)\n",
    "t_pad = int(v_padding if v_padding % 1 == 0 else v_padding+0.5)\n",
    "r_pad = int(h_padding if h_padding % 1 == 0 else h_padding-0.5)\n",
    "b_pad = int(v_padding if v_padding % 1 == 0 else v_padding-0.5)\n",
    "padding = torchvision.transforms.v2.Pad(padding=(l_pad, t_pad, r_pad, b_pad), fill=tuple(int(x*255) for x in image_processor.image_mean))\n",
    "padded = padding(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[122, 122, 122,  ..., 122, 122, 122],\n",
      "         [122, 122, 122,  ..., 122, 122, 122],\n",
      "         [122, 122, 122,  ..., 122, 122, 122],\n",
      "         ...,\n",
      "         [122, 122, 122,  ..., 122, 122, 122],\n",
      "         [122, 122, 122,  ..., 122, 122, 122],\n",
      "         [122, 122, 122,  ..., 122, 122, 122]],\n",
      "\n",
      "        [[116, 116, 116,  ..., 116, 116, 116],\n",
      "         [116, 116, 116,  ..., 116, 116, 116],\n",
      "         [116, 116, 116,  ..., 116, 116, 116],\n",
      "         ...,\n",
      "         [116, 116, 116,  ..., 116, 116, 116],\n",
      "         [116, 116, 116,  ..., 116, 116, 116],\n",
      "         [116, 116, 116,  ..., 116, 116, 116]],\n",
      "\n",
      "        [[104, 104, 104,  ..., 104, 104, 104],\n",
      "         [104, 104, 104,  ..., 104, 104, 104],\n",
      "         [104, 104, 104,  ..., 104, 104, 104],\n",
      "         ...,\n",
      "         [104, 104, 104,  ..., 104, 104, 104],\n",
      "         [104, 104, 104,  ..., 104, 104, 104],\n",
      "         [104, 104, 104,  ..., 104, 104, 104]]], dtype=torch.uint8)\n",
      "336\n"
     ]
    }
   ],
   "source": [
    "print(padded)\n",
    "print(image_processor.crop_size['height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 336, 336])\n",
      "torch.Size([3, 1000, 1000])\n",
      "tensor([[[0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         ...,\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784]],\n",
      "\n",
      "        [[0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         ...,\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549]],\n",
      "\n",
      "        [[0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         ...,\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078]]])\n",
      "tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "         ...,\n",
      "         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],\n",
      "\n",
      "        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "         ...,\n",
      "         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
      "\n",
      "        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "         ...,\n",
      "         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])\n"
     ]
    }
   ],
   "source": [
    "resized = resize(padded, size=[image_processor.crop_size['height'], image_processor.crop_size['width']])\n",
    "print(resized.shape)\n",
    "cropped = torchvision.transforms.v2.functional.center_crop(img, output_size=[w, w])\n",
    "print(cropped.shape)\n",
    "scaled = resized * image_processor.rescale_factor\n",
    "print(scaled)\n",
    "normed = torchvision.transforms.v2.functional.normalize(scaled, image_processor.image_mean, image_processor.image_std)\n",
    "print(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(padded[:][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48145466, 0.4578275, 0.40821073]\n",
      "[0.26862954, 0.26130258, 0.27577711]\n",
      "0.00392156862745098\n"
     ]
    }
   ],
   "source": [
    "print(image_processor.image_mean)\n",
    "print(image_processor.image_std)\n",
    "print(image_processor.rescale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:\n",
      "torch.Size([3, 667, 1000])\n",
      "prehout shape:\n",
      "torch.Size([3, 336, 336])\n",
      "type(imagt) = <class 'torch.Tensor'>, imagt.dtype = torch.float16, imagt.shape = torch.Size([3, 667, 1000])\n"
     ]
    }
   ],
   "source": [
    "pre_config = PreprocessConfig(image_processor.crop_size, image_processor.image_mean, image_processor.image_std,\n",
    "                              image_processor.rescale_factor)\n",
    "preprocessor = Preprocess(pre_config)\n",
    "preout = preprocessor(imagt)\n",
    "# plot([imagt])\n",
    "# plt.imshow(imagt)\n",
    "\n",
    "print(\"input shape:\")\n",
    "print(imagt.shape)\n",
    "print(\"prehout shape:\")\n",
    "print(preout.shape)\n",
    "\n",
    "print(f\"{type(imagt) = }, {imagt.dtype = }, {imagt.shape = }\")\n",
    "\n",
    "inputs = (imagt,)\n",
    "# ex_program = capture_pre_autograd_graph(model, inputs)\n",
    "# dynamic_shapes = [{0: torch.export.Dim('length', min=1, max=2048),1: torch.export.Dim('width', min=1, max=2048)}]\n",
    "length = Dim('length', min=8, max=4090)\n",
    "width = Dim('width', min=10, max=4092)  # 1346 <= 2*_width <= 2048\n",
    "# width = 2*_width\n",
    "dynamic_shapes = [{1: length, 2: width}]\n",
    "\n",
    "encoder = Encoder(model)\n",
    "\n",
    "model = EncoderModel(preprocessor, encoder)\n",
    "# enout = encoder(inputs)\n",
    "ex_encodermodel = torch.export.export(model, inputs, dynamic_shapes=dynamic_shapes, strict=False)\n",
    "torch.export.save(ex_encodermodel, './pre_encoder.pt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "\n",
    "eager_out = model(inputs[0])\n",
    "export_out = ex_encodermodel.module()(inputs[0])\n",
    "torch.allclose(eager_out, export_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.90it/s]\n",
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When visiting this location, which features a pier extending over a large body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as the pier may be affected by strong winds or storms, which could make it unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could pose a risk to your safety. Additionally, be cautious of the tides and currents, as they can change rapidly and may pose a danger to swimmers or boaters. Finally, be respectful of the environment and other visitors, and follow any posted rules or guidelines to ensure a safe and enjoyable experience for everyone.\n"
     ]
    }
   ],
   "source": [
    "# eager mode inference\n",
    "eval_model(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking inside the model object from load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llava.model.language_model.llava_llama.LlavaLlamaForCausalLM"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llava.model.language_model.llava_llama.LlavaLlamaModel"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.get_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llava-v1.5-7b'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_name_from_path(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.constants import (\n",
    "    IGNORE_INDEX,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "def get_prompt(query: str, mm_use_im_start_end: bool, model_name: str) -> str:\n",
    "    qs = query\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in qs:\n",
    "        if mm_use_im_start_end:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "        else:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "    else:\n",
    "        if mm_use_im_start_end:\n",
    "            qs = image_token_se + \"\\n\" + qs\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "    def get_conv_mode(model_name: str) -> str:\n",
    "        if \"llama-2\" in model_name.lower():\n",
    "            conv_mode = \"llava_llama_2\"\n",
    "        elif \"mistral\" in model_name.lower():\n",
    "            conv_mode = \"mistral_instruct\"\n",
    "        elif \"v1.6-34b\" in model_name.lower():\n",
    "            conv_mode = \"chatml_direct\"\n",
    "        elif \"v1\" in model_name.lower():\n",
    "            conv_mode = \"llava_v1\"\n",
    "        elif \"mpt\" in model_name.lower():\n",
    "            conv_mode = \"mpt\"\n",
    "        else:\n",
    "            conv_mode = \"llava_v0\"\n",
    "        return conv_mode\n",
    "    conv = conv_templates[get_conv_mode(model_name)].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_prompt(args.query, model.config.mm_use_im_start_end, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "What are the things I should be cautious about when I visit here? ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_tensor(args) -> torch.Tensor:\n",
    "    image_files = args.image_file.split(args.sep)\n",
    "    images = load_images(image_files)\n",
    "    image_sizes = [x.size for x in images]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "    return image_sizes, images_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes, image_tensor = get_image_tensor(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cpu()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(input_ids))\n",
    "print(type(image_sizes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inside prepare_inputs_labels_for_multimodal\n",
    "\n",
    "attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "labels = torch.full_like(input_ids, IGNORE_INDEX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   319, 13563,  1546,   263, 12758,  5199,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  5199, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  5618,   526,\n",
      "           278,  2712,   306,   881,   367,   274,  1300,  2738,  1048,   746,\n",
      "           306,  6493,  1244, 29973,   319,  1799,  9047, 13566, 29901]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.llava_arch import LlavaMetaForCausalLM\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PreprocessConfig:\n",
    "    crop_size: dict\n",
    "    image_mean: list[float]\n",
    "    image_std: list[float]\n",
    "    rescale_factor: float\n",
    "\n",
    "class TextImageTokenEmbedding(torch.nn.Module):\n",
    "    def __init__(self, llava_model: LlavaMetaForCausalLM, config: PreprocessConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model_ = llava_model\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model_.get_model()\n",
    "\n",
    "    def encode_images(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        images = images.to(dtype=torch.float16)\n",
    "        image_features = self.get_model().get_vision_tower()(images)\n",
    "        image_features = self.get_model().mm_projector(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def image_preprocess(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        w = max(img.shape[1], img.shape[2])\n",
    "        # pad the image with median rgb value, to make a square\n",
    "        h_padding = (w - img.shape[1]) / 2\n",
    "        v_padding = (w - img.shape[2]) / 2\n",
    "        l_pad = int(h_padding if h_padding % 1 == 0 else h_padding+0.5)\n",
    "        t_pad = int(v_padding if v_padding % 1 == 0 else v_padding+0.5)\n",
    "        r_pad = int(h_padding if h_padding % 1 == 0 else h_padding-0.5)\n",
    "        b_pad = int(v_padding if v_padding % 1 == 0 else v_padding-0.5)\n",
    "        padding = torchvision.transforms.v2.Pad(padding=(l_pad, t_pad, r_pad, b_pad), fill=tuple(int(x*255) for x in image_processor.image_mean))\n",
    "        padded = padding(img)\n",
    "        # here padded shape should be max(h, w) x max(h, w)\n",
    "        resized = resize(padded, size=[self.config.crop_size['height'], self.config.crop_size['width']])\n",
    "        torch._check(resized.size(1) == self.config.crop_size['height'])\n",
    "        torch._check(resized.size(2) == self.config.crop_size['width'])\n",
    "        # print(resized.shape)\n",
    "        # cropped = torchvision.transforms.v2.functional.center_crop(img, output_size=[w, w])\n",
    "        # print(cropped.shape)\n",
    "        scaled = resized * self.config.rescale_factor\n",
    "        # print(scaled)\n",
    "        normed = torchvision.transforms.v2.functional.normalize(scaled, self.config.image_mean, self.config.image_std)\n",
    "        # print(normed)\n",
    "        return normed.unsqueeze(0)\n",
    "    \n",
    "    def prepare_inputs_labels_for_multimodal_one_image(self, input_ids: torch.Tensor, images: torch.Tensor) -> torch.Tensor:\n",
    "        assert isinstance(input_ids, torch.Tensor), f\"Expecting input_ids to be a tensor, got {input_ids}\"\n",
    "        assert input_ids.shape[0] == 1, f\"Expecting input_ids to be of shape [1, num_tokens], got {input_ids.shape}\"\n",
    "        input_ids = input_ids.squeeze(0)\n",
    "        # preprocessed_img = self.image_preprocess(imagt)\n",
    "        # preprocessed_img = torch.unsqueeze(preprocessed_img, dim=0)\n",
    "\n",
    "        index = torch.where(input_ids == IMAGE_TOKEN_INDEX)[0]\n",
    "        input_before_img = self.get_model().embed_tokens(input_ids[:index]).unsqueeze(0)\n",
    "        input_after_img = self.get_model().embed_tokens(input_ids[index+1:]).unsqueeze(0)\n",
    "\n",
    "        image_embeds = self.encode_images(images)\n",
    "        # new_input_embeds = torch.cat(input_embeds, image_embeds)\n",
    "        result = torch.cat((input_before_img, image_embeds, input_after_img), dim=1)\n",
    "        return result\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, images: torch.Tensor) -> torch.Tensor:\n",
    "        preprocessed_img = self.image_preprocess(images)\n",
    "        return self.prepare_inputs_labels_for_multimodal_one_image(input_ids, preprocessed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 667, 1000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms._functional_tensor import resize\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "# from helpers import plot\n",
    "\n",
    "from torch.export import export\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torch.export import Dim\n",
    "from dataclasses import dataclass\n",
    "\n",
    "imagr = torchvision.io.read_image(image_files[0])\n",
    "imagt = imagr.to(dtype=torch.float16)\n",
    "print(imagt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_config = PreprocessConfig(image_processor.crop_size, image_processor.image_mean, image_processor.image_std,\n",
    "                              image_processor.rescale_factor)\n",
    "embedding = TextImageTokenEmbedding(model, pre_config)\n",
    "embeds = embedding.forward(input_ids, imagr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, compare, _ = model.prepare_inputs_labels_for_multimodal(input_ids, None, None, None, None, image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(embeds, compare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])\n",
      "tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "preprocessed_img = embedding.image_preprocess(imagr)\n",
    "print(preprocessed_img[:, :, :, 0])\n",
    "print(image_tensor.to(dtype=torch.float32)[:, :, :, 0])\n",
    "print(torch.allclose(preprocessed_img.to(dtype=torch.float16)[:, :, :, 0], image_tensor[:, :, :, 0], rtol=1e-03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " tensor([120, 120, 120, 121, 121, 121, 122, 122, 122, 122, 123, 123, 123, 124,\n",
       "         124, 125, 125, 126, 126, 126, 139, 181, 181, 188, 188, 123, 181, 181,\n",
       "         181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181,\n",
       "         181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181,\n",
       "         181, 181, 181, 188, 188, 188, 188, 188, 188, 188, 188, 188, 134, 181,\n",
       "         181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181,\n",
       "         181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181,\n",
       "         181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 188, 188]),\n",
       " tensor([ 43,  44,  45,  43,  44,  45,  44,  45,  46,  47,  45,  46,  47,  48,\n",
       "          49,  54,  55,  53,  54,  55,  45, 155, 156, 154, 155,  46, 101, 103,\n",
       "         104, 105, 106, 107, 108, 109, 110, 112, 114, 115, 119, 124, 125, 126,\n",
       "         127, 128, 133, 134, 135, 139, 140, 141, 142, 145, 146, 149, 150, 151,\n",
       "         154, 155, 156, 151, 152, 153, 154, 155, 156, 157, 158, 159, 269, 101,\n",
       "         102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
       "         119, 124, 125, 126, 127, 128, 130, 132, 133, 134, 135, 136, 137, 139,\n",
       "         140, 141, 142, 143, 145, 146, 149, 150, 155, 156, 154, 155]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = torch.abs(preprocessed_img.to(dtype=torch.float16) - image_tensor)\n",
    "torch.where(diff > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 634, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(image_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_input_embeds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([input_embeds, image_embeds])\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(new_input_embeds\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "new_input_embeds = torch.cat([input_embeds, image_embeds])\n",
    "print(new_input_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "          [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "          [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "          ...,\n",
      "          [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "          [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
      "          [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],\n",
      "\n",
      "         [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          ...,\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
      "\n",
      "         [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          ...,\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[166., 166., 166.,  ..., 150., 149., 149.],\n",
      "         [166., 166., 166.,  ..., 150., 149., 149.],\n",
      "         [166., 166., 166.,  ..., 150., 150., 149.],\n",
      "         ...,\n",
      "         [ 38.,  38.,  38.,  ...,  29.,  29.,  29.],\n",
      "         [ 38.,  38.,  38.,  ...,  29.,  29.,  29.],\n",
      "         [ 38.,  38.,  38.,  ...,  29.,  29.,  29.]],\n",
      "\n",
      "        [[193., 193., 193.,  ..., 181., 180., 180.],\n",
      "         [193., 193., 193.,  ..., 181., 180., 180.],\n",
      "         [193., 193., 193.,  ..., 181., 181., 180.],\n",
      "         ...,\n",
      "         [ 61.,  61.,  61.,  ...,  51.,  51.,  51.],\n",
      "         [ 61.,  61.,  61.,  ...,  51.,  51.,  51.],\n",
      "         [ 61.,  61.,  61.,  ...,  51.,  51.,  51.]],\n",
      "\n",
      "        [[220., 220., 220.,  ..., 210., 209., 209.],\n",
      "         [220., 220., 220.,  ..., 210., 209., 209.],\n",
      "         [220., 220., 220.,  ..., 210., 210., 209.],\n",
      "         ...,\n",
      "         [ 75.,  75.,  75.,  ...,  64.,  64.,  64.],\n",
      "         [ 75.,  75.,  75.,  ...,  64.,  64.,  64.],\n",
      "         [ 75.,  75.,  75.,  ...,  64.,  64.,  64.]]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(imagt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
