{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "from torch import nn\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, tokenizer_image_token\n",
    "from llava.eval.run_llava import eval_model, load_images, process_images\n",
    "# from executorch.exir import EdgeProgramManager, ExecutorchProgramManager, to_edge\"\n",
    "\n",
    "from llava.model.multimodal_encoder.builder import build_vision_tower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "image = Image.open(requests.get('https://llava-vl.github.io/static/images/view.jpg', stream=True).raw)\n",
    "# temp_file = \"/Users/larryliu/Downloads/pyturkeys.jpg\"\n",
    "temp_file = \"./view.jpg\"\n",
    "image.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 667, 1000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms._functional_tensor import resize\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "# from helpers import plot\n",
    "\n",
    "from torch.export import export\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torch.export import Dim\n",
    "from dataclasses import dataclass\n",
    "\n",
    "imagr = torchvision.io.read_image(temp_file)\n",
    "print(imagr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[166, 166, 166,  ..., 150, 149, 149],\n",
      "         [166, 166, 166,  ..., 150, 149, 149],\n",
      "         [166, 166, 166,  ..., 150, 150, 149],\n",
      "         ...,\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29],\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29],\n",
      "         [ 38,  38,  38,  ...,  29,  29,  29]],\n",
      "\n",
      "        [[193, 193, 193,  ..., 181, 180, 180],\n",
      "         [193, 193, 193,  ..., 181, 180, 180],\n",
      "         [193, 193, 193,  ..., 181, 181, 180],\n",
      "         ...,\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51],\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51],\n",
      "         [ 61,  61,  61,  ...,  51,  51,  51]],\n",
      "\n",
      "        [[220, 220, 220,  ..., 210, 209, 209],\n",
      "         [220, 220, 220,  ..., 210, 209, 209],\n",
      "         [220, 220, 220,  ..., 210, 210, 209],\n",
      "         ...,\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64],\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64],\n",
      "         [ 75,  75,  75,  ...,  64,  64,  64]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(imagr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.82it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "prompt = \"What are the things I should be cautious about when I visit here?\"\n",
    "image_files = [temp_file]  # IMG_3997\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"query\": prompt,\n",
    "    \"conv_mode\": None,\n",
    "    \"image_file\": image_files[0],\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512\n",
    "})()\n",
    "\n",
    "model_name = get_model_name_from_path(args.model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    args.model_path, args.model_base, model_name, device_map=\"cpu\", device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.constants import (\n",
    "    IGNORE_INDEX,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "import re\n",
    "\n",
    "def get_prompt(query: str, mm_use_im_start_end: bool, model_name: str) -> str:\n",
    "    qs = query\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in qs:\n",
    "        if mm_use_im_start_end:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "        else:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "    else:\n",
    "        if mm_use_im_start_end:\n",
    "            qs = image_token_se + \"\\n\" + qs\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "    def get_conv_mode(model_name: str) -> str:\n",
    "        if \"llama-2\" in model_name.lower():\n",
    "            conv_mode = \"llava_llama_2\"\n",
    "        elif \"mistral\" in model_name.lower():\n",
    "            conv_mode = \"mistral_instruct\"\n",
    "        elif \"v1.6-34b\" in model_name.lower():\n",
    "            conv_mode = \"chatml_direct\"\n",
    "        elif \"v1\" in model_name.lower():\n",
    "            conv_mode = \"llava_v1\"\n",
    "        elif \"mpt\" in model_name.lower():\n",
    "            conv_mode = \"mpt\"\n",
    "        else:\n",
    "            conv_mode = \"llava_v0\"\n",
    "        return conv_mode\n",
    "    conv = conv_templates[get_conv_mode(model_name)].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_tensor(args) -> torch.Tensor:\n",
    "    image_files = args.image_file.split(args.sep)\n",
    "    images = load_images(image_files)\n",
    "    image_sizes = [x.size for x in images]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "    return image_sizes, images_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 166.5\n",
      "0 167 0 166\n",
      "torch.Size([3, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "img = imagr\n",
    "w = max(img.shape[1], img.shape[2])\n",
    "v_padding = (w - img.shape[1]) / 2\n",
    "h_padding = (w - img.shape[2]) / 2\n",
    "print(h_padding, v_padding)\n",
    "l_pad = int(h_padding if h_padding % 1 == 0 else h_padding+0.5)\n",
    "t_pad = int(v_padding if v_padding % 1 == 0 else v_padding+0.5)\n",
    "r_pad = int(h_padding if h_padding % 1 == 0 else h_padding-0.5)\n",
    "b_pad = int(v_padding if v_padding % 1 == 0 else v_padding-0.5)\n",
    "print(l_pad, t_pad, r_pad, b_pad)\n",
    "padding = torchvision.transforms.v2.Pad(padding=(l_pad, t_pad, r_pad, b_pad), fill=tuple(int(x*255) for x in image_processor.image_mean))\n",
    "padded = padding(img)\n",
    "print(padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "336\n"
     ]
    }
   ],
   "source": [
    "print(image_processor.crop_size['height'])\n",
    "print(image_processor.crop_size['width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 336, 336])\n",
      "torch.Size([3, 1000, 1000])\n",
      "tensor([[[0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         ...,\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784],\n",
      "         [0.4784, 0.4784, 0.4784,  ..., 0.4784, 0.4784, 0.4784]],\n",
      "\n",
      "        [[0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         ...,\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.4549, 0.4549, 0.4549]],\n",
      "\n",
      "        [[0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         ...,\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078]]])\n",
      "torch.Size([3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "# interpolation=\"bicubic\" -> PIL image.resize(resample=3)\n",
    "resized = resize(padded, size=[image_processor.crop_size['height'], image_processor.crop_size['width']], interpolation=\"bicubic\")\n",
    "print(resized.shape)\n",
    "cropped = torchvision.transforms.v2.functional.center_crop(img, output_size=[w, w])\n",
    "print(cropped.shape)\n",
    "scaled = resized * image_processor.rescale_factor\n",
    "print(scaled)\n",
    "normed = torchvision.transforms.v2.functional.normalize(scaled, image_processor.image_mean, image_processor.image_std)\n",
    "print(normed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.transforms.functional.to_pil_image(resized)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48145466, 0.4578275, 0.40821073]\n",
      "[0.26862954, 0.26130258, 0.27577711]\n",
      "0.00392156862745098\n"
     ]
    }
   ],
   "source": [
    "print(image_processor.image_mean)\n",
    "print(image_processor.image_std)\n",
    "print(image_processor.rescale_factor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking inside the model object from load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llava.model.language_model.llava_llama.LlavaLlamaForCausalLM"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llava.model.language_model.llava_llama.LlavaLlamaModel"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.get_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llava-v1.5-7b'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_name_from_path(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_prompt(args.query, model.config.mm_use_im_start_end, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "What are the things I should be cautious about when I visit here? ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes, image_tensor = get_image_tensor(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cpu()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   319, 13563,  1546,   263, 12758,  5199,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  5199, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  5618,   526,\n",
      "           278,  2712,   306,   881,   367,   274,  1300,  2738,  1048,   746,\n",
      "           306,  6493,  1244, 29973,   319,  1799,  9047, 13566, 29901]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(input_ids == -200)[0].dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.llava_arch import LlavaMetaForCausalLM\n",
    "from transformers import LlamaForCausalLM\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class PreprocessConfig:\n",
    "    crop_size: dict\n",
    "    image_mean: list[float]\n",
    "    image_std: list[float]\n",
    "    rescale_factor: float\n",
    "\n",
    "class TextImageTokenEmbedding(torch.nn.Module):\n",
    "    def __init__(self, llava_model: LlavaMetaForCausalLM, config: PreprocessConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model_ = llava_model\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model_.get_model()\n",
    "\n",
    "    def encode_images(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        images = images.to(dtype=torch.float16)\n",
    "        image_features = self.get_model().get_vision_tower()(images)\n",
    "        image_features = self.get_model().mm_projector(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def image_preprocess(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        w = max(img.shape[1], img.shape[2])\n",
    "        # pad the image with median rgb value, to make a square\n",
    "        v_padding = (w - img.shape[1]) / 2\n",
    "        h_padding = (w - img.shape[2]) / 2\n",
    "        l_pad = int(math.ceil(h_padding))\n",
    "        t_pad = int(math.ceil(v_padding))\n",
    "        r_pad = int(math.floor(h_padding))\n",
    "        b_pad = int(math.floor(v_padding))\n",
    "        padded = torchvision.transforms.v2.functional.pad(img, padding=(l_pad, t_pad, r_pad, b_pad), fill=tuple(int(x*255) for x in image_processor.image_mean))\n",
    "        # here padded shape should be max(h, w) x max(h, w)\n",
    "        resized = resize(padded, size=[image_processor.crop_size['height'], image_processor.crop_size['width']], interpolation=\"bicubic\")\n",
    "        torch._check(resized.size(1) == self.config.crop_size['height'])\n",
    "        torch._check(resized.size(2) == self.config.crop_size['width'])\n",
    "        # print(resized.shape)\n",
    "        # cropped = torchvision.transforms.v2.functional.center_crop(img, output_size=[w, w])\n",
    "        # print(cropped.shape)\n",
    "        scaled = resized * self.config.rescale_factor\n",
    "        # print(scaled)\n",
    "        normed = torchvision.transforms.v2.functional.normalize(scaled, self.config.image_mean, self.config.image_std)\n",
    "        # print(normed)\n",
    "        return normed.unsqueeze(0)\n",
    "    \n",
    "    def prepare_inputs_labels_for_multimodal_one_image(self, prompt_before_image: torch.Tensor, images: torch.Tensor, prompt_after_image: torch.Tensor) -> torch.Tensor:\n",
    "        assert isinstance(input_ids, torch.Tensor), f\"Expecting input_ids to be a tensor, got {input_ids}\"\n",
    "        assert input_ids.shape[0] == 1, f\"Expecting input_ids to be of shape [1, num_tokens], got {input_ids.shape}\"\n",
    "        prompt_before_image = prompt_before_image.squeeze(0)\n",
    "        prompt_after_image = prompt_after_image.squeeze(0)\n",
    "\n",
    "        # preprocessed_img = self.image_preprocess(imagt)\n",
    "        # preprocessed_img = torch.unsqueeze(preprocessed_img, dim=0)\n",
    "\n",
    "        embeds_before_img = self.get_model().embed_tokens(prompt_before_image).unsqueeze(0)\n",
    "        embeds_after_img = self.get_model().embed_tokens(prompt_after_image).unsqueeze(0)\n",
    "\n",
    "        image_embeds = self.encode_images(images)\n",
    "        # new_input_embeds = torch.cat(input_embeds, image_embeds)\n",
    "        result = torch.cat((embeds_before_img, image_embeds, embeds_after_img), dim=1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, prompt_before_image: torch.Tensor, images: torch.Tensor, prompt_after_image: torch.Tensor) -> torch.Tensor:\n",
    "        preprocessed_img = self.image_preprocess(images)\n",
    "        embeds = self.prepare_inputs_labels_for_multimodal_one_image(prompt_before_image, preprocessed_img, prompt_after_image)\n",
    "        return LlamaForCausalLM.forward(self.model_, inputs_embeds=embeds, return_dict=False, use_cache=False, output_hidden_states=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_config = PreprocessConfig(image_processor.crop_size, image_processor.image_mean, image_processor.image_std,\n",
    "                              image_processor.rescale_factor)\n",
    "embedding = TextImageTokenEmbedding(model, pre_config)\n",
    "preprocessed_img = embedding.image_preprocess(imagr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, compare, _ = model.prepare_inputs_labels_for_multimodal(input_ids, None, None, None, None, image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(preprocessed_img.to(dtype=torch.float16), image_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 1.3506,  0.7173, -0.3391,  ...,  2.0527,  2.9883,  1.7217],\n",
      "         [-9.5781, -5.0586, -1.2363,  ..., -6.3008, -8.3516, -7.7188],\n",
      "         [-5.5156, -6.3789,  8.1094,  ..., -3.5781, -1.9883, -2.7910],\n",
      "         ...,\n",
      "         [-5.5820, -2.0820,  8.6484,  ..., -1.2207, -3.9805, -2.7949],\n",
      "         [-3.2168, -1.6426,  7.6055,  ...,  0.6445, -0.4446,  1.1699],\n",
      "         [-1.2051, -1.5215,  9.7344,  ...,  1.5684,  1.8301,  1.8594]]],\n",
      "       grad_fn=<ToCopyBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "logits = embedding.forward(input_ids, imagr)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 634, 32000])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_state[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m next_token_logits \u001b[39m=\u001b[39m hidden_state[\u001b[39m0\u001b[39m][:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m      2\u001b[0m next_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(next_token_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(next_tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_state' is not defined"
     ]
    }
   ],
   "source": [
    "next_token_logits = hidden_state[0][:, -1, :]\n",
    "next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "print(next_tokens)\n",
    "print(tokenizer.decode(next_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaConfig {\n",
      "  \"_name_or_path\": \"liuhaotian/llava-v1.5-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaLlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"freeze_mm_mlp_adapter\": false,\n",
      "  \"freeze_mm_vision_resampler\": false,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"image_aspect_ratio\": \"pad\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mm_hidden_size\": 1024,\n",
      "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
      "  \"mm_resampler_type\": null,\n",
      "  \"mm_use_im_patch_token\": false,\n",
      "  \"mm_use_im_start_end\": false,\n",
      "  \"mm_vision_select_feature\": \"patch\",\n",
      "  \"mm_vision_select_layer\": -2,\n",
      "  \"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"tune_mm_mlp_adapter\": false,\n",
      "  \"tune_mm_vision_resampler\": false,\n",
      "  \"unfreeze_mm_vision_tower\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_mm_proj\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35])\n",
      "torch.Size([1, 23])\n"
     ]
    }
   ],
   "source": [
    "length = Dim('length', min=8, max=4091)\n",
    "token_dim_1 = Dim('token_dim_1', min=2, max=3518)\n",
    "token_dim_2 = Dim('token_dim_2', min=2, max=3518)\n",
    "width = Dim('width', min=9, max=4092)\n",
    "\n",
    "# width = 2*_width\n",
    "dynamic_shapes = [{1: token_dim_1}, {1: length, 2: width}, {1: token_dim_2}]\n",
    "index = torch.where(input_ids == IMAGE_TOKEN_INDEX)[1]\n",
    "prompt_before_image = input_ids[:, :index]\n",
    "print(prompt_before_image.shape)\n",
    "prompt_after_image = input_ids[:, index+1:]\n",
    "print(prompt_after_image.shape)\n",
    "inputs = (prompt_before_image, imagr, prompt_after_image)\n",
    "exported = torch.export.export(embedding, inputs, dynamic_shapes=dynamic_shapes, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = exported.module()(prompt_before_image, imagr, prompt_after_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 1.3506,  0.7173, -0.3391,  ...,  2.0527,  2.9883,  1.7217],\n",
      "         [-9.5781, -5.0586, -1.2363,  ..., -6.3008, -8.3516, -7.7188],\n",
      "         [-5.5156, -6.3789,  8.1094,  ..., -3.5781, -1.9883, -2.7910],\n",
      "         ...,\n",
      "         [-5.5820, -2.0820,  8.6484,  ..., -1.2207, -3.9805, -2.7949],\n",
      "         [-3.2168, -1.6426,  7.6055,  ...,  0.6445, -0.4446,  1.1699],\n",
      "         [-1.2051, -1.5215,  9.7344,  ...,  1.5684,  1.8301,  1.8594]]],\n",
      "       grad_fn=<ToCopyBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1932])\n",
      "When\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = logits[0][:, -1, :]\n",
    "next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "print(next_tokens)\n",
    "print(tokenizer.decode(next_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(model, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/Users/larryliu/CLionProjects/executorch/llava.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaLlamaForCausalLM(\n",
      "  (model): LlavaLlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "    (vision_tower): CLIPVisionTower(\n",
      "      (vision_tower): CLIPVisionModel(\n",
      "        (vision_model): CLIPVisionTransformer(\n",
      "          (embeddings): CLIPVisionEmbeddings(\n",
      "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "            (position_embedding): Embedding(577, 1024)\n",
      "          )\n",
      "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder): CLIPEncoder(\n",
      "            (layers): ModuleList(\n",
      "              (0-23): 24 x CLIPEncoderLayer(\n",
      "                (self_attn): CLIPAttention(\n",
      "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): CLIPMLP(\n",
      "                  (activation_fn): QuickGELUActivation()\n",
      "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mm_projector): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "model.vision_tower.vision_tower.vision_model.embeddings.class_embedding\n",
      "model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight\n",
      "model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight\n",
      "model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight\n",
      "model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias\n",
      "model.vision_tower.vision_tower.vision_model.post_layernorm.weight\n",
      "model.vision_tower.vision_tower.vision_model.post_layernorm.bias\n",
      "model.mm_projector.0.weight\n",
      "model.mm_projector.0.bias\n",
      "model.mm_projector.2.weight\n",
      "model.mm_projector.2.bias\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(model.state_dict().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "for weight in model.state_dict().values():\n",
    "    print(weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.export.save(exported, './llava.pt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
